{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', 'love', 'my', 'ma', 'and', 'pa', ',', 'but', 'does', 'that', 'mean', 'I', 'should', 'marry', 'my', 'sister', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # uncomment if you haven't downloaded punkt yet\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "new_sentence = \"I do love my ma and pa, but does that mean I should marry my sister?\"\n",
    "# tokens = nltk.word_tokenize(sentence)\n",
    "tokens = nltk.word_tokenize(new_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('do', 'VBP'), ('love', 'VB'), ('my', 'PRP$'), ('ma', 'NN'), ('and', 'CC'), ('pa', 'NN'), (',', ','), ('but', 'CC'), ('does', 'VBZ'), ('that', 'IN'), ('mean', 'VB'), ('I', 'PRP'), ('should', 'MD'), ('marry', 'VB'), ('my', 'PRP$'), ('sister', 'NN'), ('?', '.')]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger') # uncomment if you haven't downloaded averaged_perceptron_tagger yet\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "print(type(tagged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,784.0,120.0\" width=\"784px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.10204%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.55102%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.10204%\" x=\"5.10204%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">do</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.65306%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"10.2041%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">love</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.2653%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"16.3265%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">my</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.3878%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.08163%\" x=\"22.449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ma</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"24.4898%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.10204%\" x=\"26.5306%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.0816%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.08163%\" x=\"31.6327%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">pa</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.6735%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.06122%\" x=\"35.7143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.2449%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.10204%\" x=\"38.7755%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">but</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.3265%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"43.8776%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">does</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.9388%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">that</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.0612%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"56.1224%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">mean</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.1837%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.10204%\" x=\"62.2449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.7959%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.16327%\" x=\"67.3469%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">should</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.4286%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.14286%\" x=\"75.5102%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">marry</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.0816%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"82.6531%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">my</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.16327%\" x=\"88.7755%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">sister</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.8571%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.06122%\" x=\"96.9388%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">?</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.4694%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('I', 'PRP'), ('do', 'VBP'), ('love', 'VB'), ('my', 'PRP$'), ('ma', 'NN'), ('and', 'CC'), ('pa', 'NN'), (',', ','), ('but', 'CC'), ('does', 'VBZ'), ('that', 'IN'), ('mean', 'VB'), ('I', 'PRP'), ('should', 'MD'), ('marry', 'VB'), ('my', 'PRP$'), ('sister', 'NN'), ('?', '.')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker') # uncomment if you haven't downloaded maxent_ne_chunker yet\n",
    "#nltk.download('words') # uncomment if you haven't downloaded words yet\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "entities\n",
    "# print(type(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "['I', 'do', 'love', 'my', 'ma', 'and', 'pa', ',', 'but', 'does', 'that', 'mean', 'I', 'should', 'marry', 'my', 'sister', '?']\n",
      "['i', 'do', 'love', 'my', 'ma', 'and', 'pa', ',', 'but', 'doe', 'that', 'mean', 'i', 'should', 'marri', 'my', 'sister', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')  # uncomment if you haven't downloaded punkt yet\n",
    "# nltk.download('averaged_perceptron_tagger')  # uncomment if you haven't downloaded averaged_perceptron_tagger yet\n",
    "# nltk.download('maxent_ne_chunker')  # uncomment if you haven't downloaded maxent_ne_chunker yet\n",
    "# nltk.download('words')  # uncomment if you haven't downloaded words yet\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "word = \"running\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)\n",
    "\n",
    "stemmed_words = [stemmer.stem(w) for w in tokens]\n",
    "print(tokens)\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "['I', 'do', 'love', 'my', 'ma', 'and', 'pa', ',', 'but', 'does', 'that', 'mean', 'I', 'should', 'marry', 'my', 'sister', '?']\n",
      "['i', 'do', 'lov', 'my', 'ma', 'and', 'pa', ',', 'but', 'doe', 'that', 'mean', 'i', 'should', 'marry', 'my', 'sist', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "word = \"running\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)\n",
    "\n",
    "stemmed_words = [stemmer.stem(w) for w in tokens]\n",
    "print(tokens)\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores: {'neg': 0.184, 'neu': 0.638, 'pos': 0.177, 'compound': -0.0258}\n",
      "Compound Sentiment: Neutral\n",
      "Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon\n",
    "#nltk.download('vader_lexicon') # uncomment if you haven't downloaded vader_lexicon yet\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Test sentence\n",
    "sentiment_sentence = \"I am using NLTK for natural language processing. i dislike some of it \"\n",
    "\n",
    "# Get sentiment scores\n",
    "sentiment_scores = sia.polarity_scores(sentiment_sentence)\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(\"Sentiment Scores:\", sentiment_scores)\n",
    "\n",
    "# Determine sentiment based on compound score\n",
    "compound_score = sentiment_scores['compound']\n",
    "\n",
    "if compound_score >= 0.05:\n",
    "    compound_sentiment = 'Positive'\n",
    "elif compound_score <= -0.05:\n",
    "    compound_sentiment = 'Negative'\n",
    "else:\n",
    "    compound_sentiment = 'Neutral'\n",
    "\n",
    "positive_score = sentiment_scores['pos']\n",
    "negative_score = sentiment_scores['neg']\n",
    "neutral_score = sentiment_scores['neu']\n",
    "\n",
    "if positive_score > negative_score and positive_score > neutral_score:\n",
    "    sentiment = 'Positive'\n",
    "elif negative_score > positive_score and negative_score > neutral_score:\n",
    "    sentiment = 'Negative'\n",
    "else:\n",
    "    sentiment = 'Neutral'\n",
    "\n",
    "# Print the final sentiment\n",
    "print(\"Compound Sentiment:\", compound_sentiment)\n",
    "print(\"Sentiment:\", sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma: dog\n",
      "Synonyms: ['dog', 'domestic_dog', 'Canis_familiaris', 'frump', 'dog', 'dog', 'cad', 'bounder', 'blackguard', 'dog', 'hound', 'heel', 'frank', 'frankfurter', 'hotdog', 'hot_dog', 'dog', 'wiener', 'wienerwurst', 'weenie', 'pawl', 'detent', 'click', 'dog', 'andiron', 'firedog', 'dog', 'dog-iron', 'chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'dog', 'go_after', 'track']\n",
      "Antonyms: []\n",
      "Hypernyms: ['canine', 'domestic_animal', 'unpleasant_woman', 'chap', 'villain', 'sausage', 'catch', 'support', 'pursue']\n",
      "Hyponyms: ['basenji', 'corgi', 'cur', 'dalmatian', 'great_pyrenees', 'griffon', 'hunting_dog', 'lapdog', 'leonberg', 'mexican_hairless', 'newfoundland', 'pooch', 'poodle', 'pug', 'puppy', 'spitz', 'toy_dog', 'working_dog', 'perisher', 'vienna_sausage', 'hound', 'quest', 'run_down', 'tree']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# WordNet Lemmatizer\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a word\n",
    "word = \"dogs\"\n",
    "lemma = lemmatizer.lemmatize(word)\n",
    "print(\"Lemma:\", lemma)\n",
    "\n",
    "# Get synonyms of a word\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(\"Synonyms:\", synonyms)\n",
    "\n",
    "# Get antonyms of a word\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "print(\"Antonyms:\", antonyms)\n",
    "\n",
    "# Get hypernyms of a word\n",
    "hypernyms = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    for hypernym in syn.hypernyms():\n",
    "        hypernyms.append(hypernym.name().split('.')[0])\n",
    "print(\"Hypernyms:\", hypernyms)\n",
    "\n",
    "# Get hyponyms of a word\n",
    "hyponyms = []\n",
    "for syn in wordnet.synsets(word):\n",
    "    for hyponym in syn.hyponyms():\n",
    "        hyponyms.append(hyponym.name().split('.')[0])\n",
    "print(\"Hyponyms:\", hyponyms)\n",
    "\n",
    "\n",
    "#Can optimise the above code by disabling duplicate entries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
