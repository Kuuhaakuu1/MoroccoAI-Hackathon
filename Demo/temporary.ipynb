{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import os\n",
    "import streamlit as st\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space \n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "path = \"المقاولة والأعمال\\الصفقات العمومية\"\n",
    "\n",
    "# Side bar contents\n",
    "with st.sidebar:\n",
    "    st.title('الذكاء الاصطناعي أشبال')\n",
    "    st.markdown('''\n",
    "    ## معلومات عنا:\n",
    "    هذا التطبيق هو تواصل مع الذكاء الاصطناعي أشبال مدعومة ب (LLM) تم بناؤه باستخدام:\n",
    "    - [Streamlit](https://streamlit.io/)\n",
    "    - [OpenAI](https://platform.openai.com/docs/models) LLM Model\n",
    "    - [Idarati dataset](https://www.idarati.ma/)\n",
    "    ''')\n",
    "    add_vertical_space(5)\n",
    "    st.write('تم إنشاؤه من قبل فريق أشبال')\n",
    "st.title('تواصل مع الذكاء الاصطناعي أشبال')\n",
    "\n",
    "\n",
    "\n",
    "if \"messages\" not in st.session_state.keys(): # Initialize the chat messages history\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\",  \"content\": \"اسألني سؤالاً حول مكتبة مستندات مفتوحة المصدر Idarati !\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_index():\n",
    "    with st.spinner(text=\"جاري تحميل  مستندات اداراتي  انتظر قليلاً! قد يستغرق هذا الأمر من 1 إلى 2 دقيقة.\"):\n",
    "        if not os.path.exists(\"./storage\"):\n",
    "            # load the documents and create the index\n",
    "            documents = SimpleDirectoryReader(path).load_data()\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "            # store it for later\n",
    "            index.storage_context.persist()\n",
    "        else:\n",
    "            # load the existing index\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        return index\n",
    "index = load_index()\n",
    "\n",
    "\n",
    "if \"chat_engine\" not in st.session_state.keys(): # Initialize the chat engine\n",
    "        st.session_state.chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "# either way we can now query the index\n",
    "# query_engine = index.as_query_engine()\n",
    "\n",
    "if prompt := st.chat_input(\" أدخل سؤالك هنا\"): # Prompt for user input and save to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "for message in st.session_state.messages: # Display the prior chat messages\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "        \n",
    "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"جارٍ التفكير...\"):\n",
    "            response = st.session_state.chat_engine.chat(prompt)\n",
    "            st.write(response.response)\n",
    "            message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "            st.session_state.messages.append(message) # Add response to message history        \n",
    "# query = st.text_input(\"What would you like to know about your PDF?\")\n",
    "    \n",
    "# if query:\n",
    "#     print(type(query))\n",
    "#     response = query_engine.query(query)\n",
    "#     st.write(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
